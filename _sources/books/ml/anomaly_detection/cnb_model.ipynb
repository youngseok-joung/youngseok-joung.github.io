{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de103ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cnb_model.py: \n",
    "\n",
    "This model is the implementation of Gaussian Naive Bayes Classification of KDD datasets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'Youngseok Joung'\n",
    "__copyright__ = \"Copyright 2007, The Cogent Project\"\n",
    "__credits__ = [\"Youngseok Joung\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Youngseok Joung\"\n",
    "__email__ = \"none\"\n",
    "__status__ = \"Production\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as splitter\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "import cProfile\n",
    "import pstats\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot\n",
    "import pickle\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b22324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelEncoding(model_name, data):\n",
    "    for column in data.columns:\n",
    "        # If the data type of the cell is 'object'(Categorical), it will be transformed as a numerical \n",
    "        if data[column].dtype == type(object):\n",
    "            le_file_path = 'result/' + model_name + '/' + model_name + '_' + column + '_encoder.pkl'\n",
    "#             print(os.path.exists(le_file_path))\n",
    "            if os.path.exists(le_file_path):\n",
    "                pkl_file = open(le_file_path, 'rb')\n",
    "                le = pickle.load(pkl_file) \n",
    "                pkl_file.close()\n",
    "                data[column] = le.transform(data[column])            \n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                data[column] = le.fit_transform(data[column])\n",
    "                #exporting the departure encoder\n",
    "                output = open(le_file_path, 'wb')\n",
    "                pickle.dump(le, output)\n",
    "                output.close()\n",
    "            if column == 'result':\n",
    "                le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "                print(le_name_mapping)\n",
    "                \n",
    "    return data, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b798dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(model_name, data):\n",
    "    y = data.result\n",
    "    x = data.drop('result', axis=1)\n",
    "    \n",
    "    # Preprocessing: Split 7:3 Train: Test\n",
    "    x_train, x_test, y_train, y_test = splitter(x, y, test_size=0.3)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7377e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model_name, x_train, x_test, y_train, y_test):\n",
    "    # Profile: Start\n",
    "    profile = cProfile.Profile()\n",
    "    profile.enable()\n",
    "    \n",
    "    # train and test\n",
    "    model = CategoricalNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Profile: End \n",
    "    profile.disable()\n",
    "    profile.dump_stats('output.prof')\n",
    "    stream = open('result/' + model_name + '/' + model_name + '_profiling.txt', 'w')\n",
    "    stats = pstats.Stats('output.prof', stream=stream)\n",
    "    stats.sort_stats('cumtime')\n",
    "    stats.print_stats()\n",
    "    os.remove('output.prof')\n",
    "    \n",
    "    # Freezing model for production\n",
    "    dump(model, 'result/' + model_name + '/' + model_name + '_model.joblib')\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d30cebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def report(model_name, y_test, y_pred, le=None):\n",
    "    \"\"\"report function evaluates the quality of the output of a classifier on this data set.\n",
    "    We can get the value of Precision, Recall,, F1-Score, Support, accuracy by Lables\n",
    "    And it can get Multiclass AUC score multiclass using roc_auc_score_multiclass function\n",
    "    Additionally, it draws Bar graph about comparison between labels in each metrics (precision, recall, f1-score, AUC)\n",
    "    All are saved as a file\n",
    "\n",
    "    :param model_name: model name used in this project (e.g. \"SVM\")\n",
    "    :param y_test: test label\n",
    "    :param y_pred: test label\n",
    "    :param le: None or Label encoder\n",
    "    :return: _confusion_matrix, _classification_report, _auc_dict, _classification_report_dict\n",
    "    \"\"\"\n",
    "\n",
    "    # Estimation: Confusion Matrix & classification-report\n",
    "    _confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "    _classification_report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=False)\n",
    "    _classification_report_dict = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)\n",
    "\n",
    "    # For Multiclass AUC\n",
    "    _auc_dict = roc_auc_score_multiclass(y_test, y_pred)\n",
    "    _auc_dict = dict((le.classes_[key], value) for (key, value) in _auc_dict.items())\n",
    "#     _auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "#     _fpr, _tpr, _thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "    with open('result/' + model_name + '/' + model_name + '_output.txt', 'w') as f:\n",
    "        f.write(\"\\n---Confusion Matrix---\\n\")\n",
    "        f.write(np.array2string(_confusion_matrix, separator=', '))\n",
    "        f.write(\"\\n---Classification Report---\\n\")\n",
    "        f.write(_classification_report)\n",
    "        f.write(\"\\n---ROC AUC Score---\\n\")\n",
    "        f.write(str(_auc_dict))\n",
    "#         f.write(_auc)\n",
    "\n",
    "    print('\\n-----Confusion Matrix-----\\n')\n",
    "    print(_confusion_matrix)\n",
    "    print('\\n-----Classification Report-----\\n')\n",
    "    print(_classification_report)\n",
    "    print('\\n-----AUC Dictionary-----\\n')\n",
    "    print(str(_auc_dict))\n",
    "\n",
    "    metrix = ['precision', 'recall', 'f1-score']\n",
    "#     metrix = ['precision', 'recall', 'f1-score', 'support']\n",
    "    xKeys = le.classes_\n",
    "    for met in metrix:\n",
    "        xValues = []\n",
    "        for target_name in le.classes_:\n",
    "            xValues += [_classification_report_dict[target_name][met]]\n",
    "\n",
    "        pyplot.title(met)\n",
    "        pyplot.bar(range(len(xValues)), list(xValues), align='center')\n",
    "        pyplot.xticks(range(len(xKeys)), list(xKeys))\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.title('AUC')\n",
    "    pyplot.bar(range(len(_auc_dict)), list(_auc_dict.values()), align='center')\n",
    "    pyplot.xticks(range(len(_auc_dict)), list(_auc_dict.keys()))\n",
    "    pyplot.show()\n",
    "\n",
    "#     # plot the roc curve for the model\n",
    "#     # pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "#     pyplot.plot(_fpr, _tpr, marker='.', label=model_name)\n",
    "#     # axis labels\n",
    "#     pyplot.xlabel('False Positive Rate')\n",
    "#     pyplot.ylabel('True Positive Rate')\n",
    "#     # show the legend\n",
    "#     pyplot.legend()\n",
    "#     # show the plot\n",
    "#     pyplot.show()\n",
    "\n",
    "    return _confusion_matrix, _classification_report, _auc_dict, _classification_report_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0344ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_multiclass(y_test, y_pred, average = \"macro\"):\n",
    "    \"\"\"roc_auc_score_multiclass function evaluate the multiclass output as a ROC AUC score.\n",
    "\n",
    "    :param y_test: test label\n",
    "    :param y_pred: test label\n",
    "    :param average: \"macro\" or Label encoder\n",
    "    :return: _confusion_matrix, _classification_report, _auc_dict, _classification_report_dict\n",
    "    \"\"\"\n",
    "\n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(y_test)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        #creating a list of all the classes except the current class\n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_y_test = [0 if x in other_class else 1 for x in y_test]\n",
    "        new_y_pred = [0 if x in other_class else 1 for x in y_pred]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_y_test, new_y_pred, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0360ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def production(model_name, data):\n",
    "    \"\"\"production function receive real network traffic data from the product\n",
    "    And classify it with saved label encoder and the model\n",
    "\n",
    "    :param model_name: model name\n",
    "    :param data: real dataset\n",
    "    :return: pred_label, real_label\n",
    "    \"\"\"\n",
    "    real_data, le = labelEncoding(model_name, data)\n",
    "    real_y = real_data.result\n",
    "    real_x = real_data.drop('result', axis=1)\n",
    "#     print(real_y)\n",
    "#     print(real_x)\n",
    "\n",
    "    clf = load('result/' + model_name + '/' + model_name + '_model.joblib')\n",
    "    yy_pred = clf.predict(real_x)\n",
    "    pred_label = le.inverse_transform(yy_pred)\n",
    "    real_label = le.inverse_transform(real_y)\n",
    "\n",
    "    return pred_label, real_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516ea60",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5550a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"Receive Input datasets\"\"\"\n",
    "    model_name = 'svm_kdd'\n",
    "    # model_name = 'svm_nsl_kdd'\n",
    "    dataset_name = 'kdd_prediction'\n",
    "    # dataset_name = 'kdd_prediction_NSL'\n",
    "\n",
    "    data = pd.read_csv('./dataset/' + dataset_name + '.csv', delimiter=',', dtype={'protocol_type': str, 'service': str, 'flag': str, 'result': str})\n",
    "#     print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a8d01f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dos': 0, 'normal': 1, 'probe': 2, 'r2l': 3, 'u2r': 4}\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"Label Encoding for categorical datasets\"\"\"\n",
    "    data, le = labelEncoding(model_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b5e2b50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    \"\"\"Pre-processing\"\"\"\n",
    "    x_train, x_test, y_train, y_test = Preprocessing(model_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a565123",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 197 is out of bounds for axis 1 with size 99",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-e532cd4f2e6e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;34m\"\"\"Train and Test\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_and_test\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-4-b5794d55a4ea>\u001B[0m in \u001B[0;36mtrain_and_test\u001B[1;34m(model_name, x_train, x_test, y_train, y_test)\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCategoricalNB\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m     \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;31m# Profile: End\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[0mcheck_is_fitted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m         \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_X\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m         \u001B[0mjll\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_joint_log_likelihood\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclasses_\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjll\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001B[0m in \u001B[0;36m_joint_log_likelihood\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m   1301\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_features_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1302\u001B[0m             \u001B[0mindices\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1303\u001B[1;33m             \u001B[0mjll\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_log_prob_\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mT\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1304\u001B[0m         \u001B[0mtotal_ll\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjll\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclass_log_prior_\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1305\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtotal_ll\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 197 is out of bounds for axis 1 with size 99"
     ]
    }
   ],
   "source": [
    "    \"\"\"Train and Test\"\"\"\n",
    "    model, y_pred = train_and_test(model_name, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea562c39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    \"\"\"Report\"\"\"\n",
    "    cm, cr, auc, _ = report(model_name, y_test, y_pred, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b62004",
   "metadata": {},
   "source": [
    "# Test in Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8ef3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    \"\"\"Production\"\"\"\n",
    "\n",
    "    real_data = pd.read_csv('./dataset/kdd_prediction.csv', delimiter=',', dtype={'protocol_type': str, 'service': str, 'flag': str, 'result': str})\n",
    "    real_data = real_data.head(1)\n",
    "\n",
    "    pred_label, real_label = production(model_name, real_data)\n",
    "    print(pred_label, real_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b274d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}